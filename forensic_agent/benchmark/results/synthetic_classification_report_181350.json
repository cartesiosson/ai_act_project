{
  "results_file": "/Users/cartesio/workspace/ai_act_project/forensic_agent/benchmark/results/benchmark_results_v1_20251230_181350.json",
  "n_incidents_type": 91,
  "n_incidents_risk": 91,
  "incident_type_classification": {
    "accuracy": 0.6153846153846154,
    "metrics": {
      "accuracy": 0.6153846153846154,
      "macro_avg": {
        "precision": 0.5841503267973857,
        "recall": 0.47307692307692306,
        "f1": 0.42831632653061225
      },
      "weighted_avg": {
        "precision": 0.7470731882496588,
        "recall": 0.6153846153846154,
        "f1": 0.5680047843313148
      },
      "per_class": {
        "appropriation": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "support": 4
        },
        "bias": {
          "precision": 0.4411764705882353,
          "recall": 1.0,
          "f1": 0.6122448979591837,
          "support": 15
        },
        "copyright": {
          "precision": 0.5555555555555556,
          "recall": 1.0,
          "f1": 0.7142857142857143,
          "support": 5
        },
        "data_leakage": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "support": 0
        },
        "discrimination": {
          "precision": 1.0,
          "recall": 0.2,
          "f1": 0.3333333333333333,
          "support": 10
        },
        "privacy_violation": {
          "precision": 0.6764705882352942,
          "recall": 0.8846153846153846,
          "f1": 0.7666666666666667,
          "support": 26
        },
        "safety_failure": {
          "precision": 1.0,
          "recall": 0.2,
          "f1": 0.3333333333333333,
          "support": 15
        },
        "transparency_failure": {
          "precision": 1.0,
          "recall": 0.5,
          "f1": 0.6666666666666666,
          "support": 16
        }
      }
    }
  },
  "risk_level_classification": {
    "accuracy": 0.8901098901098901,
    "metrics": {
      "accuracy": 0.8901098901098901,
      "macro_avg": {
        "precision": 0.44505494505494503,
        "recall": 0.5,
        "f1": 0.47093023255813954
      },
      "weighted_avg": {
        "precision": 0.7922956164714406,
        "recall": 0.8901098901098901,
        "f1": 0.8383593151035013
      },
      "per_class": {
        "HighRisk": {
          "precision": 0.8901098901098901,
          "recall": 1.0,
          "f1": 0.9418604651162791,
          "support": 81
        },
        "MinimalRisk": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "support": 10
        }
      }
    }
  },
  "ground_truth_distribution": {
    "incident_types": {
      "bias": 15,
      "discrimination": 10,
      "privacy_violation": 26,
      "safety_failure": 15,
      "transparency_failure": 16,
      "copyright": 5,
      "appropriation": 4
    },
    "risk_levels": {
      "MinimalRisk": 10,
      "HighRisk": 81
    }
  },
  "prediction_distribution": {
    "incident_types": {
      "bias": 34,
      "privacy_violation": 34,
      "safety_failure": 3,
      "transparency_failure": 8,
      "discrimination": 2,
      "copyright": 9,
      "data_leakage": 1
    },
    "risk_levels": {
      "HighRisk": 91
    }
  }
}