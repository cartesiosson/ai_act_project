
# Compliance Gap Detection Evaluation Report

**Generated:** 2025-12-11 12:35:31
**Evaluation Method:** AIAAIC Issues → EU AI Act Requirements Mapping

## Executive Summary

This evaluation measures how well the forensic agent detects compliance gaps
by comparing detected missing requirements against expected requirements
derived from AIAAIC incident issues.

### Performance Comparison


| Incidents | Precision | Recall | F1 Score |
|-----------|-----------|--------|----------|
| 10 | 0.750 | 0.661 | 0.703 |
| 20 | 0.646 | 0.569 | 0.605 |
| 50 | 0.582 | 0.545 | 0.563 |


## Detailed Results

### 10 Incidents Evaluation

| Metric | Value |
|--------|-------|
| Precision | 0.750 |
| Recall | 0.661 |
| F1 Score | 0.703 |
| True Positives | 39 |
| False Positives | 13 |
| False Negatives | 20 |


### 20 Incidents Evaluation

| Metric | Value |
|--------|-------|
| Precision | 0.646 |
| Recall | 0.569 |
| F1 Score | 0.605 |
| True Positives | 62 |
| False Positives | 34 |
| False Negatives | 47 |


### 50 Incidents Evaluation

| Metric | Value |
|--------|-------|
| Precision | 0.582 |
| Recall | 0.545 |
| F1 Score | 0.563 |
| True Positives | 145 |
| False Positives | 104 |
| False Negatives | 121 |


## Issue-Level Analysis (50 incidents)

| Issue | Precision | Recall | F1 | Count |
|-------|-----------|--------|-----|-------|
| Transparency | 0.67 | 0.57 | 0.62 | 18 |
| Accuracy/reliability | 0.58 | 0.53 | 0.55 | 18 |
| Fairness | 0.66 | 0.56 | 0.60 | 15 |
| Accountability | 0.88 | 0.64 | 0.74 | 12 |
| Mis/disinformation | 0.43 | 0.33 | 0.38 | 11 |
| Safety | 0.66 | 0.68 | 0.67 | 10 |
| Privacy/surveillance | 0.67 | 0.73 | 0.70 | 9 |
| Employment | 0.50 | 0.37 | 0.42 | 6 |
| Security | 0.57 | 0.65 | 0.60 | 3 |
| Human rights/civil liberties | 0.65 | 0.59 | 0.62 | 3 |
| Dual use | 0.64 | 0.82 | 0.72 | 2 |
| Privacy | 0.50 | 0.56 | 0.53 | 2 |
| Authenticity/integrity | 0.50 | 0.43 | 0.46 | 2 |
| Alignment | 0.71 | 0.50 | 0.59 | 2 |
| Normalisation | 0.33 | 0.14 | 0.20 | 1 |
| Ethics/values | 0.43 | 0.43 | 0.43 | 1 |


## Requirement Detection Analysis (50 incidents)

| Requirement | Expected | Detected | Rate |
|-------------|----------|----------|------|
| transparencyrequirement | 28 | 18 | 64.3% |
| robustnessrequirement | 25 | 11 | 44.0% |
| accuracyrequirement | 25 | 18 | 72.0% |
| gpaitransparencyrequirement | 18 | 0 | 0.0% |
| nondiscriminationrequirement | 18 | 6 | 33.3% |
| accuracyevaluationrequirement | 18 | 0 | 0.0% |
| humanoversightrequirement | 18 | 4 | 22.2% |
| documentationrequirement | 14 | 14 | 100.0% |
| biasdetectionrequirement | 14 | 7 | 50.0% |
| fairnessrequirement | 14 | 14 | 100.0% |
| safetyrequirement | 13 | 11 | 84.6% |
| loggingrequirement | 12 | 12 | 100.0% |
| riskmanagementrequirement | 11 | 2 | 18.2% |
| privacyprotectionrequirement | 11 | 11 | 100.0% |
| datagovernancerequirement | 11 | 11 | 100.0% |
| fundamentalrightsassessmentrequirement | 10 | 3 | 30.0% |
| securityrequirement | 3 | 3 | 100.0% |
| cybersecurityrequirement | 3 | 0 | 0.0% |


## Methodology

### Ground Truth Derivation
- AIAAIC Issue field indicates what compliance aspect failed
- Each Issue maps to expected EU AI Act requirements
- Example: "Transparency" → TransparencyRequirement, GPAITransparencyRequirement

### Metrics Definition
- **Precision**: Of detected gaps, how many were expected?
- **Recall**: Of expected gaps, how many were detected?
- **F1**: Harmonic mean of precision and recall

### Limitations
- AIAAIC Issues indicate failures, not necessarily EU AI Act requirements
- Some issues have indirect mappings
- Agent simulation may differ from actual LLM-based detection

## Visual Analysis

See accompanying heatmap images:
- `heatmap_10.png` - 10 incidents analysis
- `heatmap_20.png` - 20 incidents analysis
- `heatmap_50.png` - 50 incidents analysis

---
*Report generated by SERAMIS Forensic Agent Benchmark Suite*
